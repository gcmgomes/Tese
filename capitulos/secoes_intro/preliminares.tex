\section{Basic definitions}
\label{sec:basic_defs}

We denote by $[n] = \{1, \dots, n\}$.
A (\tdef{multi})\tdef{family} is a (multi)set of sets.
The \tdef{power set} $2^S$ of a set $S$ is the family of all subsets of $S$.
A \tdef{k-partition} of $S$ into $k$ sets is denoted by $S \sim \{S_1, \dots, S_k\}$ such that $S_i \cap S_j = \emptyset$ and $\bigcup_{i \leq k} S_i = S$.
A \tdef{k-(multi)cover} of $S$ is a (multi)family $\{S_1, \dots, S_k\}$ of subsets of $S$ such that $\bigcup_{i \in [k]} S_i = S$.
A (multi)family $\mathcal{F}$ satisfies the \tdef{Helly condition} or \tdef{Helly property} if and only if, for every pairwise intersecting subfamily $\mathcal{F}'$ of $\mathcal{F}$, $\bigcap_{F \in \mathcal{F}'} F \neq \emptyset$.
The \tdef{intersection graph} of a multifamily $\mathcal{F} \subseteq 2^S$, denoted by $G = \Omega(\mathcal{F})$ is the graph of order $|\mathcal{F}|$ and, for every $F_u, F_v \in \mathcal{F}$, $uv \in E(G) \Leftrightarrow F_u \cap F_v \neq \emptyset$.
Any $\mathcal{F}$ such that $\Omega(\mathcal{F}) \simeq G$ is a \tdef{set representation} of $G$.
A known theorem states that every graph is the intersection graph of a family of subgraphs of a graph~\citep{intersection_graphs}.

A \tdef{simple graph} of \tdef{order} (or \tdef{size}) $n$ is an ordered pair $G = (V(G), E(G))$, where $V(G)$ is its \tdef{vertex set} of cardinality $n$ and its \tdef{edge set}, $E(G)$, is a family of pairs of distinct elements of $V(G)$. A graph is \tdef{trivial} if $|V(G)| = 1$.
Instead of $\{u,v\} \in E(G)$, we denote an edge by $uv$, simply due to convenience. Moreover, when there is no ambiguity, we denote $V(G)$ by $V$, $E(G)$ by $E$, $|V|$ as $n$ and $|E|$ as $m$.

We say that two vertices $u,v \in V$ are \tdef{adjacent} or \tdef{neighbors} if $uv \in E$.
A graph $G' = (V', E')$ is a \tdef{subgraph} of $G$ if $V' \subseteq V$ and $E' \subseteq E$.
If $E' = \{uv \in E \mid u,v \in V'\}$ we say that $V'$ \tdef{induces} $G'$, $G' = G[V']$ and that $G'$ is the \tdef{induced subgraph} of $G$ by $V$.
For simplicity, we denote by $G - v$ the graph $G[V(G) \setminus \{v\}]$ and, similarly, for $S \subseteq V(G)$, $G \setminus S$ is equivalent to $G[V(G) \setminus S]$.

The \tdef{open neighborhood}, or just \tdef{neighborhood} of a vertex $v$ in $G$ is given by $N_G(v) = \{u \mid uv \in E(G)\}$, its \tdef{closed neighborhood} by $N_G[v] = N_G(v) \cup \{v\}$ and its \tdef{degree} by $\deg_G(v) = |N_G(v)|$.
A vertex is \tdef{simplicial} if its neighbors are pairwise adjacent.
For a set $S \subseteq V$, we denote its open and closed neighborhood as $N_G(S) = \bigcup_{v \in S} N_G(v) \setminus S$, $N_G[S] = N_G(S) \cup S$ and $\deg_G(S) = |N_G(S)|$, respectively.
The \tdef{complement} $\overline{G}$ of $G$ is defined as $V(\overline{G}) = V(G)$ and $E(\overline{G}) = \{uv | uv \notin E\}$.
Given a graph $G$, we denote its \tdef{maximum degree} by $\Delta(G)$ and \tdef{minimum degree} by $\delta(G)$.

Two vertices $u,v$ are \tdef{false twins} if $N_G(u) = N_G(v)$ and \tdef{true twins} if $N_G[u] = N_G[v]$. $u,v$ are of the same \tdef{type} if they are either true or false twins.
Being of the same type is an equivalence relation~\citep{neighborhood_diversity}, and the number of different types on a graph $G$ is called its \tdef{neighborhood diversity}, $\nd(G)$.

Two graphs $G$ and $H$ are \tdef{isomorphic} if and only if there is a bijection $f: V(G) \mapsto V(H)$ such that $uv \in E(G) \Leftrightarrow f(u)f(v) \in E(H)$.
We denote isomorphism by $G \simeq H$.
A graph $G$ is said to be \tdef{free} of a graph $H$, or $H$-free, if there is no induced subgraph $G'$ of $G$ such that $G'$ and $H$ are isomorphic.

The \tdef{path} of length $k$, or $P_k$, is a graph with $k$ vertices $v_1 \dots v_k$ such that $v_iv_j \in E(P_k)$ if and only if $j = i+1$. Moreover, we say that $v_1, v_k$ are the \tdef{extremities}, or \tdef{endvertices}, of $P_k$ and all other $v_i$ are its \tdef{inner} vertices.
The length of a path is the number of edges contained in it, that is, $P_k$ has length $k-1$.
An \tdef{induced path} of $G$ is a subgraph $G'$ of $G$ that is isomorphic to a path.
A \tdef{cycle} with  $k \geq 3$ vertices is a path with $k$ vertices plus the edge $v_kv_1$; analogously, the length of a cycle is defined as the number of edges it contains.
An \tdef{induced cycle} of $G$ is an induced subgraph $G'$ of $G$ that is isomorphic to a cycle.
A \tdef{chord} in a cycle $C$ of length at least 4 is an edge between two non-consecutive vertices of $C$.
The \tdef{girth} of $G$, denoted by $\girth(G)$, is the length of the smallest induced cycle of $G$.
A \tdef{hole} is a chordless cycle of length at least 4; it is an even-hole if it has an even number of vertices, or an odd-hole, otherwise. An \tdef{anti-hole} is the complement of a hole.
$G$ is \tdef{acyclic} if and only if there is no induced cycle in $G$.
A \tdef{matching} is a set of edges such no two share a common endpoint.
A maximum matching is said to be \textit{perfect} if every vertex of the graph is contained in one edge of the matching.

A graph $G$ is \tdef{connected} if and only if there is an induced path between every pair $u,v \in V$, and \tdef{disconnected}, otherwise.
A \tdef{connected component}, or simply a \tdef{component}, of $G$ is a maximal connected induced subgraph of $G$.
Given a graph $G$, the \tdef{distance} $\dist_G(u,v)$ between two vertices $u,v$ in $G$ is the minimum number of edges in any path between them.
If $u,v$ are in different components, we say that $\dist_G(u,v) = \infty$.
The \tdef{diameter} of a connected graph $G$ is defined as the length of the longest shortest path between any pair of vertices $u,v \in V(G)$.
The \tdef{$k$-th power} $G^k$ of a graph $G$ is the graph where $V(G^k) = V(G)$ and $E(G^k) = \{uv \mid \dist_G(u,v) \leq k\}$.
When $k = 2$, $G^2$ is also called the \tdef{square} of graph $G$ and $G$ is called the \tdef{square root} of $G^2$.
The class of all graphs that admit a square root is called \tdef{square graphs}.
When the graph in question is clear, we will omit the $G$ subscript.

An \tdef{articulation point}, \tdef{cut point} or \tdef{cut vertex} of a connected graph $G$ is a vertex $v$ such that $G - v$ is disconnected.
A \tdef{bridge} is an edge of $G$ whose removal increases the number of connected components of $G$.
$G$ is \tdef{biconnected} if $G$ is connected and does not have a cut vertex.
A \tdef{cutset} of a connected graph $G$ is a set $S \subset V$ such that $G \setminus S$ is disconnected.
In particular, a cut vertex is a cutset of size one.

The \tdef{complete} graph $K_n$ of order $n$ is a graph where every pair of vertices is adjacent.
A \tdef{clique} of $G$ of size $n$ is a set $S \subseteq V$ such that $G[S]$ is isomorphic to $K_n$.
Similarly, an \tdef{independent set} of $G$ of size $n$ is a set $S \subseteq V$ such that $G[S]$ is isomorphic to $\overline{K_n}$.
We denote by $\omega(G)$ and $\alpha(G)$ the size of the maximum induced clique and maximum independent set of a given $G$.
An \tdef{edge clique cover} $\mathcal{Q} = \{Q_1, \dots, Q_n\}$ of a graph $G$ is a (multi)family of cliques of $G$ such that every edge of $G$ is contained in at least one element of $\mathcal{Q}$.

A graph is a \tdef{cluster graph} if all of its connected components are cliques.
Analogously, a graph is a \tdef{co-cluster graph} if its complement is a cluster graph.
The \tdef{distance to cluster} (resp. \tdef{co-cluster}) of a graph $G$, denoted by $\dc(G)$ (resp. $\dcc(G)$), is the size of the smallest subset of vertices $U$ of $G$ such that $G - U$ is a (co-)cluster graph.
These parameters can be computed in $\bigO{1.92^{\dc( G )}n^2 }$
time and $\bigO{1.92^{\dcc( G )}n^2 }$ time, respectively~\citep{branching-cluster}.
It is quite easy, however, to obtain a 3-approximation for them in polynomial time, it suffices to note that a graph is a cluster graph if and only if it is $P_3$-free: while there is some $P_3$ in the graph, it suffices to remove all three vertices.
The above values are examples of \tdef{structural graph parameters}.
Determining certain parameters of a generic graph $G$ is efficient (such as $\Delta(G)$ and $\delta(G)$); however, others (such as $\omega(G)$ and $\alpha(G)$) are widely believed to be hard to ascertain.

A graph $G$ is \tdef{bipartite} if $V(G) \sim \{X, Y\}$ such that both $X$ and $Y$ are independent sets.
Such property implies that a graph is bipartite if and only if it is $C_{2k+1}$-free, for any $k \geq 1$.
A \tdef{biclique} $K_{n_1,n_2}$ is a bipartite graph with $|X| = n_1$, $|Y| = n_2$ and $uv \in E(G)$ for every pair $u \in X$ and $v \in Y$.
A \tdef{star} is a biclique with $|X| = 1$ and $|Y| \geq 1$ and its \textit{center} is the vertex of maximum degree.
Clearly, we can also define \tdef{induced bicliques} and \tdef{induced stars} much like induced cliques.
A graph is \tdef{multipartite} if $V(G) \sim \{X_1, \dots,  X_p\}$ and $X_i$ is an independent set for all $i$; it is a \tdef{complete multipartite} graph if $uv \in E(G)$ whenever $u \in X_i$, $v \in X_j$ and $i \neq j$.

A \tdef{hypergraph} $\hypergraph = (V, \mathcal{E})$ is a natural generalization of a graph.
That is, $V(\hypergraph)$ is its vertex set and $\mathcal{E} \subseteq 2^{V}$ its \tdef{hyperedge} set~\citep{hypergraphs}.
A graph $G$ is said to be a \tdef{host} of $\mathcal{H}$ if $V(G) = V(\hypergraph)$, every hyperedge of $\hypergraph$ induces a connected subgraph of $G$ and every edge of $G$ is contained in at least one hyperedge of $\mathcal{H}$.
A hypergraph is \tdef{$k$-uniform} if all of its hyperedges have the same size $k$.

A \tdef{transversal} of a hypergraph $\hypergraph$ is a set $X \subseteq V(\hypergraph)$ such that, for every hyperedge $\varepsilon \in \mathcal{E}(\hypergraph)$, $X \cap \varepsilon \neq \emptyset$.
If $X$ is not a transversal we say that it is an \tdef{oblique}.

The \tdef{clique hypergraph} $\Hyper{C}(G)$ of a graph $G$ is the hypergraph on the same vertex set of $G$ and with hyperedge set equal to the family of maximal cliques of $G$.
Similarly, the \tdef{biclique hypergraph} $\Hyper{B}(G)$ of a graph $G$ is the hypergraph on the same vertex set of $G$ and with hyperedge set equal to the family of maximal bicliques of $G$.

A \tdef{tree decomposition} of a graph $G$ is defined as the pair $\mathbb{T} = \left(T, \mathcal{B} = \{B_j \mid j \in V(T)\}\right)$, where $T$ is a tree and $\mathcal{B} \subseteq 2^{V(G)}$ is a family satisfying $\bigcup_{B_j \in \mathcal{B}} B_j = V(G)$~\citep{treewidth};
for every edge $uv \in E(G)$ there is some~$B_j$ such that $\{u,v\} \subseteq B_j$;
for every $i,j,q \in V(T)$, if $q$ is in the path between $i$ and $j$ in $T$, then $B_i \cap B_j \subseteq B_q$.
Each $B_j \in \mathcal{B}$ is called a \emph{bag} of the tree decomposition.
The \emph{width} of a tree decomposition is defined as the size of a largest bag minus one.
The \emph{treewidth} $\tw(G)$ of a graph $G$ is the smallest width among all valid tree decompositions of $G$~\citep{downey_fellows}.
If $\mathbb{T}$ is a rooted tree, by $G_x$ we will denote the subgraph of $G$ induced by the vertices contained in any bag that belongs to the subtree of $\mathbb{T}$ rooted at bag $x$.
An algorithmically useful property of tree decompositions is the existence of a so called \emph{nice tree decompositions} of width $\tw(G)$.

\begin{class_definition*}[Nice tree decomposition]
    A tree decomposition $\mathbb{T}$ of $G$ is said to be \emph{nice} if it is a tree rooted at, say, the empty bag $r(T)$ and each of its bags is from one of the following four types:
    \begin{enumerate}
        \item \emph{Leaf node}: a leaf $x$ of $\mathbb{T}$ with $B_x = \emptyset$.
        \item \emph{Introduce node}: an inner bag $x$ of $\mathbb{T}$ with one child $y$ such that $B_x \setminus B_y = \{u\}$.
        \item \emph{Forget node}: an inner bag $x$ of $\mathbb{T}$ with one child $y$ such that $B_y \setminus B_x = \{u\}$.
        \item \emph{Join node}: an inner bag $x$ of $\mathbb{T}$ with two children $y,z$ such that $B_x = B_y = B_z$.
    \end{enumerate}
\end{class_definition*}

\section{Parameterized complexity}

We discuss problems in different complexity classes; in particular, we work with the usual classes $\P$, $\NP$, and the \tdef{polynomial hierarchy}~\citep{polynomial_hierarchy}.
We say that an algorithm is \tdef{efficient} if its running time is bounded by a polynomial on the size of the input and that a problem belonging to $\NPH$ is most likely intractable.
As such, our complexity results will be given either by efficient algorithms or polynomial reductions from $\NPH$ problems.

A problem being \NPH\ means that we believe that exists no \textit{exact} algorithm that runs in \textit{polynomial time} for \textit{all instances}.
Nevertheless, these hard problems are usually the ones we are most interested in, as many of them model almost perfectly practical problems such as vehicle routing~\citep{vrp} and code compilation~\citep{compilers}.
To cope with this hardness, algorithm designers usually give up on one of the three requirements of the perfect algorithm.
If the optimality of the feasible solution is not as crucial but we want to solve whichever instance comes our way, we can make use of heuristics and metaheuristics~\citep{heuristics}, which usually yield no guarantee on the quality of the solution, or, if such a guarantee is desired, approximation algorithms~\citep{approximation}.
On the other hand, if an exact solution is a must have, we may give up on the polynomial time constraint and use some quite powerful all-purpose tools such as integer linear optimization~\citep{linear_optimization}, or design ad-hoc exact exponential algorithms~\citep{exact_exponential_algorithms} that use clever tricks and problem properties to reduce the exponential factor as much as possible.

All of the above areas have a rich literature with results on hundreds upon hundreds of problems.
A much newer field -- known as \textit{parameterized complexity}, or \textit{multivariate complexity} -- arises when we sacrifice the constraint to solve all instances with a \textit{single} algorithm in exchange for polynomial time and optimality.
In parameterized complexity, algorithms are designed and analyzed not only with respect to the size of the input object, but also with other \tdef{parameters} of the input, which come in all sorts of flavors.
Many decision problems usually have some integer quantity representing a constraint of the problem, such as the minimum/maximum size of a feasible solution; such quantities are usually called the \textit{natural parameter} of the problem.
For instance, \pname{Vertex Cover} -- one of the classical examples of success of parameterized complexity -- asks for a set of size at most $k$ of vertices covering all the edges of the graph; in this case, $k$ is the natural parameter for \pname{Vertex Cover}.
Other parameters are less problem specific and relate to the structure of the graph, such as diameter or maximum degree.
The most prominent of these examples, however, is the graph parameter treewidth, which played a pivotal role in the theory of graph minors.
Other previously discussed structural parameters include neighborhood diversity, distance to cluster and distance to co~cluster.

A problem is said to be \tdef{fixed-parameter tractable} (or \FPT) when \tdef{parameterized by} $k$ if there is an algorithm with running time $f(k)n^{\bigO{1}}$, where $n$ is the size of the input object.
We denote complexities of this form by $\bigOs{f(k)}$.
In fact, we shall use $\bigOs{\cdot}$ to omit polynomial factors of the running time; that is, an algorithm with complexity $2^{f(n)}\text{poly}(n)$ is said to execute in $\bigOs{2^{f(n)}}$.
In a slight abuse of notation, $k$ is simultaneously the parameter we are working with and the value of such parameter.
An instance of a parameterized algorithm is, therefore, the pair $(x,k)$, with $x$ the input object and $k$ as previously defined.
The class of all problems that admit an FPT algorithm is the class $\FPT$.
If an algorithm has running time $\bigO{n^{f(k)}}$, for some computable function on $k$,  we say it is an \XP\ (slicewise polynomial) algorithm, and the corresponding problem it solves is in $\XP$. 

Much like classical univariate theory, some problems do not appear to admit an \FPT\ algorithm for certain parameterizations. In particular, its widely believed that finding a clique of size $k$ in a graph, parameterized by $k$, is not in $\FPT$.
In an analogue to the classical case, hardness results are usually given by what are called \tdef{parameterized reductions}.

\begin{class_definition*}[Parameterized reduction]
    A parameterized reduction from problem $\Pi$ to problem $\Pi'$ is a transformation from an instance $(x, k)$ of $\Pi$ to an instance $(x',k')$ of $\Pi'$ such that:
    \begin{enumerate}
        \item There is a solution to $(x,k)$ if and only if there is a solution to $(x',k')$;
        \item $k' \leq g(k)$ for some computable function $g$;
        \item The transformation's running time is $\bigOs{f(k)}$.
    \end{enumerate}
\end{class_definition*}

Note that the constraints imposed by parameterized reductions are quite similar to those imposed by polynomial reductions.
We ask that $k'$ does not depend on $|x|$ - which doesn't always happen with polynomial reductions - but, at the same time, allow $\FPT$ time for the transformation, instead of the more restrictive polynomial time.
These differences imply that polynomial reductions and parameterized reductions are incomparable, with some rare cases where the transformation is both polynomial and parameterized.

Unlike the theory of $\NPcness$, where most hard problems are equivalent to each other under polynomial reductions, in parameterized complexity problems seem to be distributed along a hierarchy of difficulty.
Before handling the classes themselves, we must first define the problems of parameterized complexity that play the same role as \pname{Satisfiability} for the classical theory. 

The \tdef{depth} of a circuit is the length (in terms of number of gates) of the longest path from any one variable to the output.
The \tdef{weft} of a circuit is the the maximum number of gates with more than 2 input variables in any path from any one variable to the circuit's output.
The circuits with weft $t$ and depth $d$, denoted by \pname{WCS}$_{t,d}$, will be the fundamental problems of the $t$-th level of our hierarchy.

\pproblem{weighted circuit Satisfiability of weft $t$ and depth $d$ (\pname{WCS}$_{t,d}$)}{A Boolean circuit $C$ with $n$ variables, weft $t$ and depth $d$.}{A positive integer $k$.}{Is $C$ satisfiable with exactly $k$ variables set to $\TRUEt$?}

\begin{class_definition*}[$\W$-hierarchy]
    For $t \geq 1$, a parameterized problem $\Pi$ is in $\W[t]$ if there is a parameterized reduction from $\tsc{wsc}_{t,d}$ to it, for some $d \geq 1$. Moreover,
    \begin{equation*}
        \FPT \subseteq \W[1] \subseteq \W[2] \subseteq \cdots \subset \XP
    \end{equation*}
\end{class_definition*}


\subsection{Kernelization}

One of the broadest class of techniques to be found in the realm of computing is perhaps that of \tdef{pre-processing}.
Every real system, in one way or another, employs routines that try to prune the search space or reduce the input instance as much as possible before doing any heavy lifting.
Such is the case with most optimization suites, such as CPLEX and Gurobi, where dozens upon dozens of pre-processing methods are readily available and in many cases successfully eliminate large chunks of the input before trying to solve the integer program directly.
Furthermore, in many cases, simple heuristics or algorithms with terrible worst case running times perform surprisingly well, and, in many cases, there was no theoretically sound approach to explain this phenomenon.
This lack of work on the subject is explained by the fact that, if an instance of an $\NPH$ problem can be reduced in polynomial time to one of bounded size, then $\P = \NP$~\citep{book-kernels}.
With the advent of parameterized algorithms, however, the situation is changing drastically.
Using this framework it has become possible to derive upper an lower bounds on the sizes of the instances obtained after a set of pre-processing rules have been applied.
We define the notions of kernels and kernelization below.

\begin{class_definition*}[Kernelization]
    A  \tdef{kernelization algorithm} is an algorithm that takes as input an instance $(x,k)$ of a parameterized problem $\Pi$ and its output is an equivalent instance $(x',k')$ of $\Pi$ such that $|x'| \leq f(k)$ and $k' \leq g(k)$, for some pair of computable functions $f,g$; instance $(x',k')$ is called the \tdef{kernel} of $(x,k)$.
\end{class_definition*}

A central result in parameterized complexity is that a parameterized problem is in \FPT\ if and only if it admits a (possibly exponential) kernel~\cite{book-kernels}.
Not all kernels are equal, and a natural desire is for the best (i.e. smallest) possible kernel.
The size of the kernel is measured by the dependency of the kernel on the parameter -- that is, a kernel that satisfies $|x'| \leq 4k$ is much better than a kernel with $|x'| \leq k^2$.
If this dependency is linear, we say that we have a \tdef{linear kernel}, if $f(k)$ is a quadratic function, than the kernel is \tdef{quadratic}, and so on.
Let $k$ be the natural parameter for the following problems.
Some famous examples of problems and their kernels include: \pname{Vertex Cover}, which admits a kernel of size $2k$; \pname{Max 3-Satisfiability}, which has a kernel on $6k$ variables and $2k$ clauses; \pname{Independent Set} on planar graphs, with a kernel of size $4(k - 1)$;
meanwhile, for \pname{Dominating Set} on graphs of girth at least five, there is a cubic kernel, but no known subcubic one~\citep{cygan_parameterized, book-kernels}.

The bound on the instance size, however, can be exponential.
For instance, \pname{Matching Cut} parameterized by the number of edges crossing the cut does not have a polynomial kernel~\citep{matching_cut_ipec}.
For some time, there were no techniques to prove that a parameterized problem does \tdef{not} admit a polynomial kernel; this changed, however with the seminal work of~\cite{BodlaenderDFH09}, where the composition and distillation techniques were first discussed, being was further deepened
by~\cite{weak_composition} and \cite{BodlaenderJK14}, where weak and cross-compositions were described.
All of these techniques, however, make use of some well established hypothesis about classical complexity classes.
For instance, distillation is based on assumption that the polynomial hierarchy does not collapse to the third level; weak-composition and cross-composition rely on the hypothesis that $\NP \subseteq \coNP/\poly$.
Despite appearing strong assumptions, if either of these hypotheses fail the implications would reverberate through much of theoretical computer science, and not only parameterized complexity.

For further reading and other more insightful discussions on the subjects of parameterized complexity and kernelization, we point to~\citep{downey_fellows, cygan_parameterized, book-kernels} from where most of the given definitions come from. 

\section{Explicit running time lower bounds}

Both the theory of \NPcness\ and \W[1]-\Hness\ give us evidence that no polynomial or \FPT\ algorithm may exist for a myriad of problems.
However, simply assuming that $\P \neq \NP$ or that $\FPT \neq \W[1]$ seems to not be enough to prove statements about asymptotic lower bounds on the running time of an algorithm.
All is not lost, but we do need to make some additional complexity assumptions.

In their groundbreaking work, \cite{eth} give many key insights and tools which have been broadly used across the field of algorithms and parameterized complexity to prove that long known algorithms are probably optimal.
Specifically, they prove what is known as the \tdef{Sparsification Lemma}, described below.
A logical formula $\phi$ on $n$ variables an $m$ clauses is in \tdef{Conjunctive Normal Form} (CNF) if $\phi = \bigwedge_{i=1}^m C_i$ and every $C_i$ is a disjunction of a subset of the $2n$ possible literals.
A formula is said to be in $r$-CNF if the size of each clause is no larger than $r$.

\begin{class_definition*}[Sparsification Lemma]
    For every $\epsilon > 0$ and positive
    integer $r$, there is a constant $C = \bigO{(\frac{n}{\epsilon})^{3r}}$ so that any $r$-CNF formula $F$ with $n$ variables, can be expressed as $F = \bigvee_{i=1}^t Y_i$, where $t \leq 2^{\epsilon n}$ and each $Y_i$ is an $r$-CNF formula with every variables appearing in at most $C$ clauses.
    Moreover, this disjunction can be computed by an algorithm running in time $2^{\epsilon n}n^{\bigO{1}}$.
\end{class_definition*}

Essentially, the Sparsification Lemma implies that, when performing a polynomial reduction \pname{$r$-Satisfiability}, for \tdef{fixed $r$}, it suffices to assume the input instance on $n$ variables has $\bigO{n}$ \tdef{clauses}.
Impagliazzo and Paturi then conjecture a cornerstone of lower bound asymptotic analysis, the \tdef{Exponential Time Hypothesis}, commonly referred to as \ETH, and its strong version, known as \SETH.

\begin{class_definition*}[Exponential Time Hypothesis]
    There is a real number $s$ such that \pname{3-Satisfiability} cannot be solved in $2^{sn}(n + m)^{\bigO{1}}$ time.
\end{class_definition*}


\begin{class_definition*}[Strong Exponential Time Hypothesis]
    \pname{Satisfiability} cannot be solved in $(2 - \epsilon)^{n}(n + m)^{\bigO{1}}$ time, for any $\epsilon > 0$.
\end{class_definition*}


It is not hard to see that if \ETH\ holds, then $\P \neq \NP$.
From the moment they were first claimed, both hypothesis have been successfully applied across the literature.
\cite{eth_survey} survey some of these results.
For instance, unless the Exponential Time Hypothesis is false, there is no algorithm running in $2^{o(n)}$ time for \pname{Vertex 3-Coloring}, \pname{Dominating Set}, \pname{Independent Set}, \pname{Vertex Cover}, nor \pname{Hamiltonian Path};
\pname{Hamiltonian Cycle} in planar graphs cannot be solved in $2^{o(\sqrt{n})}n^{\bigO{1}}$ time.
Let $k$ denote the natural parameter of each of the following problems.
In terms of $\FPT$ algorithms, the existence of $2^{o(k)}n^{\bigO{1}}$ was ruled out for \pname{Vertex Cover}, \pname{Feedback Vertex Set}, and \pname{Longest Path}, while no $2^{o(\sqrt{k})}n^{\bigO{1}}$ time algorithm exists for \pname{Vertex Cover} on planar graphs.
\ETH\ can also be used to give algorithmic lower bound to problems not in $\FPT$.
\cite{eth_survey} neither \pname{Dominating Set}, \pname{Clique}, \pname{Independent Set}, nor their multicolored versions can be solved in $f(k)n^{o(k)}$.

While most of the complexity theory community believes \ETH\ to be true, the same is not true for the Strong Exponential time Hypothesis~\citep{bad_seth}.
The implications for \SETH, however, as the name suggests, are quite powerful.
While \ETH\ is generally used to prove assertions on the exponent of the running times of many algorithms, \SETH\ allows for a \textit{much} finer-grained analysis, at the cost of much more complex reductions and arguments, specially because the hypothesis of the Sparsification Lemma \textit{are not respected by \pname{Satisfiability}}.
\cite{seth_lokshtanov} give a series of reductions for many problems parameterized by treewidth.
They show that the best known algorithms parameterized by treewidth for \pname{Independent Set}, \pname{Dominating Set}, \pname{Max Cut}, \pname{Odd Cycle Transversal}, \pname{Vertex $q$-Coloring} (for any $q \geq 3$), \pname{Partition Into Triangles} cannot be improved, unless \SETH\ is false.
Recently, \cite{seth_subset_sum} proved what may surely be considered a breakthrough result: by using a hypothesis on the running time of \pname{Satsifability} (\SETH), they proved that the pseudo-polynomial dynamic programming algorithm given by \cite{bellman_dp} for \pname{Subset Sum} is optimal.


\section{Graph classes}
\label{sec:graph_classes}

Most problems in graph theory can be tackled with an arbitrary input, that is, there is no particular property that we can exploit; this can happen if the considered application is too broad or little is known about its domain.
However, it might be possible to guarantee certain characteristics for the given graph, either due to constraints of the application~\citep{fernando_chordal} or due to theoretical interest.
Regardless, such guarantees might be strong enough to provide an efficient algorithm to an otherwise $\NPH$ problem.
When constraining our analysis to certain graphs, we refer to the family of all graphs that satisfy the same properties as a \tdef{graph class}.
A subfamily of a class that satisfies additional properties is referred to as a \tdef{subclass}.
For (much) more on graph classes, \cite{classes_survey} give an extensive survey of much of the work done on the field until the late 1990s.

In this section, we review some of the most studied classes and some of their properties that will aid us in the design of our algorithms.

A graph is a \tdef{tree} $T$ if it is a connected acyclic graph or, equivalently, the connected graph such that, between every pair of vertices $u$ and $v$, there exists a unique path.
The vertices of degree one of a tree are called its \tdef{leaves}, and all others are \tdef{inner} nodes. A \tdef{subtree} $T'$ of a tree $T$ is a connected subgraph which, clearly, must also be a tree.
A \tdef{rooted tree} $T_v$ is a tree with a special vertex $v$, called its \tdef{root}.
Rooted trees offer a straight forward ordering of the vertices of a tree and a nice way to decompose problems into smaller instances and combine their solutions.
A \tdef{rooted subtree} $T_u$ of $T_v$ is the subgraph of $T_v$ induced by $u$ and all vertices of $T_v$ whose path to $v$ passes through $u$.
The vertices in $T_u \setminus \{u\}$ are called the \tdef{descendants} of $u$ and its neighbors are its \tdef{children}.

A \tdef{forest} is a graph where every connected component is a tree.
Many problems which are usually quite hard for general graphs, or even some classes, usually have a straightforward answer for forests, either using a greedy strategy or a slightly more sofisticated dynamic programming idea.

\begin{figure}
    \centering
    % Define style for nodes
    \begin{tikzpicture}[scale=0.7]
    %\draw[help lines] (-5,-5) grid (5,5);
        \def\x{-2}
        \GraphInit[vstyle=Normal]
        \SetVertexNormal[Shape=circle, FillColor=black, MinSize=2pt]
        \tikzset{VertexStyle/.append style = {inner sep = \inners, outer sep = \outers}}
        \Vertex[x=0,y=1,Lpos=270,NoLabel=true]{a}
        \Vertex[x=-1,y=3,Lpos=90,NoLabel=true]{b}
        \Vertex[x=1,y=2,Lpos=90,NoLabel=true]{c}
        \Vertex[x=-3,y=2,Lpos=90,NoLabel=true]{d}
        \Vertex[x=-2,y=-1,Lpos=90,NoLabel=true]{e}
        \Vertex[x=1,y=-1,Lpos=90,NoLabel=true]{f}
        \Vertex[x=-4,y=-1,Lpos=90,NoLabel=true]{g}
        \Vertex[x=-2,y=-3,Lpos=90,NoLabel=true]{h}
        \Vertex[x=1,y=-3,Lpos=90,NoLabel=true]{i}
        \Vertex[x=3,y=-3,Lpos=90,NoLabel=true]{j}
        \Vertex[x=2.5,y=3,Lpos=90,NoLabel=true]{k}
        
        
        \Edge(a)(b)
        \Edge(a)(c)
        \Edge(k)(c)
        \Edge(a)(d)
        \Edge(a)(e)
        \Edge(a)(f)
        \Edge(e)(g)
        \Edge(e)(h)
        \Edge(f)(i)
        \Edge(j)(i)
        
        
    \end{tikzpicture}
    \caption{A tree.}
    %  Some graph
    \label{fig:some_tree}
\end{figure}


\tdef{Chordal graphs} have many nice properties that enable the computation of different graph parameters in polynomial time~\citep{golumbic}. A \tdef{perfect elimination ordering} of a graph $G$ is an ordering $v_1, \dots, v_n$ of its vertices such that for the graph $G[\{v_i, \dots, v_n\}]$, $v_i$ is a simplicial vertex. As the name implies, chordal graphs are exactly the graphs where every cycle of size at least 4 has at least one chord; more over, the following statements are equivalent: (i) $G$ is chordal; (ii) $G$ is $C_k$-free, for every $k \geq 4$; (iii) every minimal cutset of $G$ is a clique; (iv) $G$ is the intersection graph of subtrees of a tree; (v) there is a perfect elimination ordering of the vertices of $G$.
For a chordal graph $G$, its \tdef{clique tree} is a tree $\mathcal{T}(G)$ such that: its vertex set, each of which is called a \tdef{bag}, is the set of maximal cliques of $G$, and, for every vertex $v$ of $G$, the set of bags which contains $v$ induces a subtree of $\mathcal{T}(G)$. It can be shown that such a tree satisfies property \textit{(iv)}.
For more on clique trees and other chordal graph properties please refer to~\citep{clique_tree}.
Figure~\ref{fig:some_chordal} gives an example of a chordal graph and its clique tree.
Not surprisingly, many subclasses of chordal graphs have also been studied, since even forests are chordal graphs.
A \tdef{block graph} is a chordal graph where every minimal cutset is a single vertex.
An \tdef{interval graph} is the intersection graph of a set of intervals over the real line.
A \tdef{split graph} is a graph with a vertex set that can be partitioned into a clique and an independent set.

\begin{figure}[!htb]
    \centering
    % Define style for nodes
    
        \begin{tikzpicture}
            \begin{scope}[rotate=90,scale=1]
                %\draw[help lines] (-4,-4) grid (4,4);
                \GraphInit[vstyle=Simple]
                \SetVertexSimple[Shape=circle, FillColor=black, MinSize=2pt]
                \tikzset{VertexStyle/.append style = {inner sep = \inners, outer sep = \outers}}
                \Vertex[x=0,y=2]{a}
                \Vertex[x=-1,y=3]{b}
                \Vertex[x=1,y=3]{c}
                \Vertex[x=0,y=4]{d}
                
                \Vertex[x=-1,y=1]{e}
                \Vertex[x=1,y=1]{f}
                
                \Vertex[x=-2,y=0]{h}
                \Vertex[x=0,y=0]{i}
                \Vertex[x=-1,y=-1]{j}
                
                \Vertex[x=2,y=2]{k}
                
                
                \Edge(a)(b)
                \Edge(a)(c)
                \Edge(a)(d)
                \Edge(b)(c)
                \Edge(b)(d)
                \Edge(d)(c)
                
                \Edge(c)(f)
                \Edge(c)(k)
                \Edge(k)(f)
                
                \Edge(a)(i)
                \Edge(a)(f)
                \Edge(i)(f)
                
                \Edge(e)(h)
                \Edge(e)(i)
                \Edge(e)(j)
                \Edge(h)(j)
                \Edge(i)(j)
            \end{scope}
        \end{tikzpicture}
    \hfill
        \begin{tikzpicture}
            \begin{scope}[rotate=90,scale=1]
                %\draw[help lines] (-4,-4) grid (4,4);
                \GraphInit[vstyle=Simple]
                \SetVertexSimple[Shape=circle, FillColor=black, MinSize=2pt]
                \tikzset{VertexStyle/.append style = {inner sep = \inners, outer sep = \outers}}
                
                \Vertex[x=-1.5,y=0]{c1}
                \Vertex[x=-0.5,y=0]{c2}
                \Vertex[x=0,y=1]{c3}
                \Vertex[x=0.5,y=2]{c4}
                \Vertex[x=0,y=3]{c5}
                \Vertex[x=1.5,y=2]{c6}
                
                \Edge(c1)(c2)
                \Edge(c2)(c3)
                \Edge(c3)(c4)
                \Edge(c4)(c5)
                \Edge(c4)(c6)
            \end{scope}
        \end{tikzpicture}
    \hfill
    \caption{A chordal graph and its clique tree.}
    %  Some graph
    \label{fig:some_chordal}
\end{figure}

\tdef{Cographs} are the graphs $G$ such that either $G$ or its complement is disconnected. At first glance, such property may not seem very helpful to the algorithm designer, but it is equivalent to a very nice recursive definition, first given in~\citep{cographs}.
Given two graphs $G$ and $H$, we define their \tdef{disjoint union} as the graph $G \cup H$ with $V(G \cup H)= V(G) \cup V(H)$ and $E(G \cup H) = E(G) \cup E(H)$, and their \tdef{join} as the graph $G \otimes H$ with vertex set is $V(G \otimes H)= V(G) \cup V(H)$ and edge set $E(G \otimes H)= E(G) \cup E(H) \cup \{uv \mid u \in V(G), v \in V(H)\}$.
In particular, the following statements are equivalent: (i) $G$ is a cograph; (ii) $G$ is $P_4$-free; (iii) $G$ can be constructed from isolated vertices by successively applying disjoint union and join operations.
Figure~\ref{fig:some_cograph} gives an example of a cograph.

\begin{figure}[!htb]
    \centering
    \begin{tikzpicture}[scale=0.7]
            \GraphInit[vstyle=Simple]
            \SetVertexSimple[Shape=circle, FillColor=black, MinSize=2pt]
            \tikzset{VertexStyle/.append style = {inner sep = \inners, outer sep = \outers}}
            \Vertices[unit=3]{circle}{a,b,c,d,e,f}
            \Vertex[x=0,y=0]{g}
            
            \Edge(b)(a)
            \Edge(c)(b)
            
            \Edge(e)(f)
            \Edge(e)(d)
            \Edge(f)(d)
            
            \Edge(g)(a)
            \Edge(g)(b)
            \Edge(g)(c)
            \Edge(g)(d)
            \Edge(g)(e)
            \Edge(g)(f)
        \end{tikzpicture}
    \caption{A cograph.}
    \label{fig:some_cograph}
\end{figure}

Another important class on graph theory, and one with a very long history of research, is the class of \textit{regular graphs}.
A graph $G$ is regular if all of vertices of $G$ have the same degree, and is $k$-regular if $\deg(v) = k$ for all $v \in V(G)$.
Despite its simplicity, regular graphs appear in many different scenarios, such as in the E$\Delta$CC conjecture on \pname{Equitable Coloring}, a conjecture for \pname{Internal Partitions}~\citep{internal_partition_regular6}, but even more in terms of algebraic graph theory~\citep{godsil}, a field dedicated to the analysis of many graph parameters through algebraic methods, such as spectral decompositions, graph polynomials, and interlacing.
In particular, by using the eigenvalues of the adjacency matrix of a graph, or its Laplacian matrix, it is possible to derive bounds for a large collection of parameters, such as independence number and chromatic number, usually in polynomial time.
Regular graphs, in particular, benefit greatly from this approach, with stronger results for this class when compared to other classes.


